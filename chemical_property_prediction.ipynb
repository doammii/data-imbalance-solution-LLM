{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dependencies\n",
    "# https://github.com/seyonechithrananda/bert-loves-chemistry/blob/master/environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --pre deepchem\n",
    "import deepchem\n",
    "deepchem.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install simpletransformers\n",
    "!pip install datasets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n",
    "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
    "if not 'bertviz_repo' in sys.path:\n",
    "  sys.path += ['bertviz_repo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/seyonechithrananda/bert-loves-chemistry.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries & Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from typing import List\n",
    "\n",
    "from rdkit import Chem\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline, RobertaModel, RobertaTokenizer\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "# import MoleculeNet loaders from DeepChem\n",
    "from deepchem.molnet import load_tox21\n",
    "\n",
    "# import MoleculeNet dataloder from bert-loves-chemistry fork\n",
    "from chemberta.utils.molnet_dataloader import load_molnet_dataset, write_molnet_dataset_for_chemprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# logging directories setting\n",
    "project_name = \"ChemBERTa_\"       # dataset 구분\n",
    "output_path = './output_bbbp'\n",
    "model_name = 'model_1'\n",
    "\n",
    "model_folder = os.path.join(output_path, model_name)\n",
    "\n",
    "evaluation_folder = os.path.join(output_path, model_name + '_evaluation')\n",
    "if not os.path.exists(evaluation_folder):\n",
    "    os.makedirs(evaluation_folder)\n",
    "\n",
    "# parameters setting\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "patience = 15\n",
    "optimizer = \"AdamW\"\n",
    "learning_rate = 0.00001\n",
    "manual_seed = 112\n",
    "\n",
    "print(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading & splitting\n",
    "- deepchem/molnet/load_function 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tasks, (train_df, valid_df, test_df), transformers = load_molnet_dataset(\"bbbp\", tasks_wanted=None)\n",
    "\n",
    "print(f\"train set: {train_df.shape[0]}\")\n",
    "print(f\"valid set: {valid_df.shape[0]}\")\n",
    "print(f\"test set:  {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with ChemBERTa\n",
    "- DeepChem + **RobERTa (BERT의 변형 모델)**\n",
    "- **Tokenizer**: **RobertaTokenizerFast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# log in to Weights & Biases for experiment tracking\n",
    "# !wandb login (your_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    # preds: (n_samples, n_classes) -> probability scores\n",
    "    # labels: (n_samples,) -> true labels\n",
    "    \n",
    "    pred_labels = preds.argmax(axis=1)\n",
    "    cm = confusion_matrix(labels, pred_labels)\n",
    "    \n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    \n",
    "    metrics = {\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TP\": TP,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "    }\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# wandb config\n",
    "# configure Weights & Biases logging\n",
    "wandb_kwargs = {'name' : model_name}\n",
    "\n",
    "classification_args = {\n",
    "    'evaluate_each_epoch': True,\n",
    "    'evaluate_during_training_verbose': True,\n",
    "    'evaluate_during_training': True,\n",
    "    'best_model_dir': model_folder,\n",
    "    'no_save': False,\n",
    "    'save_eval_checkpoints': False,\n",
    "    'save_model_every_epoch': False,\n",
    "    'save_best_model': True,\n",
    "    'save_steps': -1,\n",
    "    'num_train_epochs': EPOCHS,\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_patience': patience,\n",
    "    'early_stopping_delta': 0.001,\n",
    "    'early_stopping_metric': 'eval_loss',\n",
    "    'early_stopping_metric_minimize': True,\n",
    "    'early_stopping_consider_epochs': True,\n",
    "    'fp16': False,\n",
    "    'optimizer': optimizer,\n",
    "    'adam_betas': (0.95, 0.999),\n",
    "    'learning_rate': learning_rate,\n",
    "    'manual_seed': manual_seed,\n",
    "    'train_batch_size': BATCH_SIZE,\n",
    "    'eval_batch_size': BATCH_SIZE,\n",
    "    'logging_steps': len(train_df) / BATCH_SIZE,\n",
    "    'auto_weights': True,\n",
    "    'wandb_project': project_name,\n",
    "    'wandb_kwargs': wandb_kwargs,\n",
    "    'compute_metrics': compute_metrics  # Add the custom metrics function here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = ClassificationModel('roberta', 'DeepChem/ChemBERTa-77M-MLM', args=classification_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "acc_list = []\n",
    "\n",
    "train_data = pd.read_csv('C:/Users/Admin/Desktop/ChemBERTa/clintox5.csv')\n",
    "X = train_data  # 전체 데이터 (SMILES와 기타 컬럼 포함)\n",
    "y = train_data['label']\n",
    "\n",
    "fold_number=1\n",
    "for train_index, val_index in skf.split(X,y):\n",
    "    train_df = train_data.iloc[train_index]\n",
    "    val_df = train_data.iloc[val_index]\n",
    "    \n",
    "    fold_output_dir = os.path.join(model_folder, f\"fold_{fold_number}\")\n",
    "    \n",
    "    results = model.train_model(train_df, eval_df=valid_df, output_dir=fold_output_dir)\n",
    "    #results = model.train_model(train_df, eval_df=valid_df, output_dir=model_folder)\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=sklearn.metrics.accuracy_score)\n",
    "    print(result['acc'])\n",
    "    acc_list.append(result['acc'])\n",
    "    fold_number += 1\n",
    "\n",
    "for i, result in enumerate(acc_list, 1):\n",
    "    print(f\"Fold-{i}: {result}\")\n",
    "    \n",
    "print(f\"{n}-fold CV accuracy result: Mean: {np.mean(acc_list)} Standard deviation:{np.std(acc_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the best model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.accuracy_score)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Data for the line graphs\n",
    "x = ['Base', 'Interval 1', 'Interval 2', 'Interval 3', 'Interval 4', 'Interval 5']\n",
    "(a1, a2, a3, a4, a5, a6) = (0.922,0.941,0.936,0.945,0.947,0.949)\n",
    "(b1, b2, b3, b4, b5, b6) = (1.0,0.315,0.612,0.685,0.704,0.712)\n",
    "(c1, c2, c3, c4, c5, c6) = (0.0,0.524,0.894,0.881,0.780,0.747)\n",
    "\n",
    "y_accuracy = [a1, a2, a3, a4, a5, a6]\n",
    "y_sensitivity = [b1, b2, b3, b4, b5, b6]\n",
    "y_specificity = [c1, c2, c3, c4, c5, c6]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each line with specified colors and labels\n",
    "plt.plot(x, y_accuracy, label='Accuracy', color='black', marker='o')\n",
    "plt.plot(x, y_sensitivity, label='Sensitivity', color='blue', marker='o')\n",
    "plt.plot(x, y_specificity, label='Specificity', color='red', marker='o')\n",
    "\n",
    "# Annotate each point with its y-value\n",
    "for i, txt in enumerate(y_accuracy):\n",
    "    plt.text(i, y_accuracy[i] + 0.03, f'{txt:.3f}', color='black', fontsize=8, ha='center')\n",
    "for i, txt in enumerate(y_sensitivity):\n",
    "    plt.text(i, y_sensitivity[i] + 0.03, f'{txt:.3f}', color='blue', fontsize=8, ha='center')\n",
    "for i, txt in enumerate(y_specificity):\n",
    "    plt.text(i, y_specificity[i] + 0.015, f'{txt:.3f}', color='red', fontsize=8, ha='center')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Intervals')\n",
    "plt.ylabel('Values')\n",
    "plt.title('BBBP - (Accuracy, Sensitivity, Specificity)')   # Dataset 구분\n",
    "plt.legend()\n",
    "\n",
    "# Set y-axis ticks\n",
    "plt.yticks([i * 0.1 for i in range(11)])\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"aplot3.png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
